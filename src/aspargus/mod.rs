use anyhow;
use chksum_hash_md5 as md5;
use chrono::{DateTime, Utc};
use ollama_rs::generation::completion::request::GenerationRequest;
use ollama_rs::generation::images::Image;
use ollama_rs::generation::options::GenerationOptions;
use ollama_rs::Ollama;
use serde::{Deserialize, Serialize};
use serde_json;
use std::collections::HashSet;
use std::io::ErrorKind;
use std::path::{Path, PathBuf};
use std::process::{Command, Stdio};
use std::sync::{Arc, Mutex};
use std::{fmt, fs};
mod image_resizer;
use base64::prelude::*;
use rayon::prelude::*;
use regex::Regex;

use self::settings::AspargusSettings;
pub(crate) mod file_management;
mod settings;

/// Represents a video resume, generated by an LLM.
/// ### Fields
/// - `title`: The title of the video.
/// - `description`: The description of the video.
/// - `keywords`: An array of keywords representing the video.
#[derive(Default, Deserialize, Serialize, Debug)]
struct Resume {
    title: String,
    description: String,
    keywords: Vec<String>,
}

/// Represents a video.
/// ### Fields
/// - `id`: An idea for this video, internal purpose.
/// - `path`: The path of the video file.
/// - `story`: The story of this video generated by the CV model when a 2 steps approach is prefered.
/// - `resume`: The resume of the video generated by the CV and optionally the text models.
/// - `thumbnails`: The thumbnails of the video.
/// - `creation_date`: The creation date of the video.
/// - `gap`: The gap between thumbnails to be extracted, based on the video's duration.
/// - `numeric_id`: The number of the video in the queue.
///
#[derive(Default, Serialize)]

pub struct Video {
    #[serde(skip_serializing)]
    id: String,
    path: String,
    #[serde(skip_serializing)]
    story: String,
    resume: Resume,
    #[serde(skip_serializing)]
    thumbnails: Vec<String>,
    #[serde(skip_serializing)]
    creation_date: DateTime<Utc>,
    #[serde(skip_serializing)]
    gap: i32,
    #[serde(skip_serializing)]
    numeric_id: i32,
    #[serde(skip_serializing)]
    skip: bool,
}

impl Video {
    /// Creates a new Video.
    ///
    /// ### Parameters
    /// - `path`: The path of the video file.
    /// - `numeric_id`: The number of the video in the queue.
    ///
    /// ### Returns
    /// A new Video.
    pub fn new(path: String, numeric_id: i32) -> Self {
        let id = md5::hash(&path).to_hex_lowercase();
        let (duration, creation_date) = get_video_metadata(path.as_str()).unwrap_or_default();
        let gap = get_capture_gap(duration.unwrap_or_default());
        Self {
            id,
            path,
            story: String::default(),
            resume: Resume::default(),
            thumbnails: Vec::new(),
            creation_date: creation_date.unwrap_or_default(),
            gap,
            numeric_id,
            skip: false,
        }
    }
}

/// Represents an Aspargus instance.
///
/// ### Fields
/// - `videos`: An array of videos to be analysed.
/// - `settings`: The Aspargus settings loaded from a file.
/// - `cv_ollama`: The computer vision model prompter.
/// - `text_ollama`: The text model prompter.
/// - `videos_number`: The number of videos in the queue.
pub(crate) struct Aspargus {
    videos: Vec<Video>,
    settings: AspargusSettings,
    cv_ollama: Ollama,
    text_ollama: Ollama,
    videos_number: i32,
}

impl Aspargus {
    /// Creates a new Aspargus instance and creates the work folders/new settings file if needed. It also loads the Aspargus settings.
    /// ### Returns
    /// A new Aspargus instance.
    pub fn new() -> Self {
        let settings = settings::load_settings();
        let computer_vision_server = settings.computer_vision_server.clone();
        let computer_vision_server_port = settings.computer_vision_server_port.clone();
        let text_server = settings.text_server.clone();
        let text_server_port = settings.text_server_port.clone();
        log::debug!("Temp folder: {}", settings.temp_folder);
        Self {
            videos: Vec::new(),
            settings,
            cv_ollama: Ollama::new(computer_vision_server, computer_vision_server_port),
            text_ollama: Ollama::new(text_server, text_server_port),
            videos_number: 0,
        }
    }

    /// Sets the computer vision model name. This name can be obtain by running '''ollama list'''.
    /// ### Parameters
    /// - `model`: The name of the computer vision model.
    pub fn set_computer_vision_model(&mut self, model: String) {
        if self.settings.computer_vision_model != model {
            self.settings.computer_vision_model = model;
            match settings::save_settings(&self.settings) {
                Ok(_) => (),
                Err(error) => log::error!("{}", error),
            }
        }
    }

    /// Sets the text model name. This name can be obtain by running '''ollama list'''.
    /// ### Parameters
    /// - `model`: The name of the text model.
    pub fn set_text_model(&mut self, model: String) {
        if self.settings.text_model != model {
            self.settings.text_model = model;
            match settings::save_settings(&self.settings) {
                Ok(_) => (),
                Err(error) => log::error!("{}", error),
            }
        }
    }

    /// Sets the computer vision server address.
    /// ### Parameters
    /// - `server`: The IP of the computer vision server.
    pub fn set_computer_vision_server(&mut self, server: String) {
        if self.settings.computer_vision_server != server {
            self.settings.computer_vision_server = server;
            match settings::save_settings(&self.settings) {
                Ok(_) => (),
                Err(error) => log::error!("{}", error),
            }
        }
    }

    /// Sets the computer vision server port.
    /// ### Parameters
    /// - `server`: The port of the computer vision server.
    pub fn set_computer_vision_server_port(&mut self, port: u16) {
        if self.settings.computer_vision_server_port != port {
            self.settings.computer_vision_server_port = port;
            match settings::save_settings(&self.settings) {
                Ok(_) => (),
                Err(error) => log::error!("{}", error),
            }
        }
    }

    /// Sets the text server address.
    /// ### Parameters
    /// - `server`: The IP of the text server.
    pub fn set_text_server(&mut self, server: String) {
        if self.settings.text_server != server {
            self.settings.text_server = server;
            match settings::save_settings(&self.settings) {
                Ok(_) => (),
                Err(error) => log::error!("{}", error),
            }
        }
    }

    /// Sets the textserver port.
    /// ### Parameters
    /// - `server`: The port of the text server.
    pub fn set_text_server_port(&mut self, port: u16) {
        if self.settings.text_server_port != port {
            self.settings.text_server_port = port;
            match settings::save_settings(&self.settings) {
                Ok(_) => (),
                Err(error) => log::error!("{}", error),
            }
        }
    }

    /// Sets the two steps flag.
    /// ### Parameters
    /// - `two_steps`: The two steps flag.
    pub fn set_two_steps(&mut self, two_steps: bool) {
        if self.settings.two_steps != two_steps {
            self.settings.two_steps = two_steps;
            match settings::save_settings(&self.settings) {
                Ok(_) => (),
                Err(error) => log::error!("{}", error),
            }
        }
    }

    pub fn is_two_steps(&mut self) -> bool {
        self.settings.two_steps
    }

    /// Add a whole list of videos to be analysed to Aspargus.
    /// ### Parameters
    /// - `paths`: The paths of the videos to analyse.
    pub fn add_videos(&mut self, paths: Vec<String>) {
        for path in paths {
            self.add_video(path);
            self.videos_number += 1;
        }
    }

    /// Gets a new numeric ID for a video.
    /// ### Returns
    /// A new numeric ID for a video.
    fn get_new_video_numeric_id(&mut self) -> i32 {
        if self.videos.len() >= 1 {
            self.videos.last().unwrap().numeric_id + 1
        } else {
            1
        }
    }

    /// Add a video to be analysed to Aspargus.
    /// ### Parameters
    /// - `path`: The path of the video to analyse.
    pub fn add_video(&mut self, path: String) {
        let the_path = Path::new(path.as_str());
        if the_path.is_file() {
            let video = Video::new(path, self.get_new_video_numeric_id());
            self.videos.push(video);
        } else {
            log::warn!(
                "File {} doesn't exist or is not a file, and therefore will be ignored.",
                path
            );
        }
    }

    /// Extract frames for all the videos in the list in the Aspargus struct.
    pub fn extract_frames(&mut self) -> anyhow::Result<()> {
        let error_holder = Arc::new(Mutex::new(None));
        self.videos.par_iter_mut().for_each(|video| {
            log::info!(
                "{}/{} - Extracting frames for {}",
                video.numeric_id,
                self.videos_number,
                video.path
            );
            match extract_frames_for_video(self.settings.temp_folder.as_str(), video) {
                Ok(thumbnails) => {
                    video.thumbnails = thumbnails;
                    //extract_faces_from_thumbnails(thumbnails);
                }
                Err(error) =>  {
                    if let Some(extraction_error) = error.downcast_ref::<FrameExtractionError>() {
                        match extraction_error {
                            FrameExtractionError::FFMpegNotFoundError(_) => {
                                let mut holder = error_holder.lock().unwrap();
                                if holder.is_none() { // Only capture the first error
                                    *holder = Some(anyhow::anyhow!("FFMpeg is not found, we're quitting for now. Please install FFMpeg and FFProbe and put them in the path."));
                                }
                            },
                            FrameExtractionError::ExtractionError(_) => {
                                video.skip = true;
                                log::error!("{}/{} - Error while extracting frames for: {}, it won't be processed further on.", video.numeric_id, self.videos_number, error)
                            },
                        }
                    } else {
                        log::error!("{}/{} - Error while extracting frames for: {}, it won't be processed further on.", video.numeric_id, self.videos_number, error)
                    }
                },
            }
        });
        let mut locked_error: std::sync::MutexGuard<Option<anyhow::Error>> =
            error_holder.lock().unwrap();
        if let Some(err) = locked_error.take() {
            Err(err)
        } else {
            Ok(())
        }
    }

    /// Runs the computer vision model for all the videos files. Note that this method must be run before the '''run_resume_model''' method.
    pub async fn run_computer_vision_model(&mut self) {
        for video in &mut self.videos {
            if video.skip {
                log::info!(
                    "{}/{} - Skipping {}",
                    video.numeric_id,
                    self.videos_number,
                    video.path
                );
            } else {
                log::info!(
                    "{}/{} - Running computer vision model for {}",
                    video.numeric_id,
                    self.videos_number,
                    video.path
                );
                match run_computer_vision_model_for_video(
                    &self.cv_ollama,
                    &self.settings.computer_vision_model,
                    video,
                )
                .await
                {
                    Ok(story) => video.story = story,
                    Err(error) => log::error!(
                        "{}/{} - Error while running computer vision model: {}",
                        video.numeric_id,
                        self.videos_number,
                        error
                    ),
                }
            }
        }
    }

    /// Runs the computer vision model for all the videos files that is able to provide a full result without running the second step with the resume model.
    pub async fn run_only_computer_vision_model(&mut self) {
        for video in &mut self.videos {
            if video.skip {
                log::info!(
                    "{}/{} - Skipping {}",
                    video.numeric_id,
                    self.videos_number,
                    video.path
                );
            } else {
                log::info!(
                    "{}/{} - Running computer vision model for {}",
                    video.numeric_id,
                    self.videos_number,
                    video.path
                );
                match run_only_computer_vision_model_for_video(
                    &self.cv_ollama,
                    &self.settings.computer_vision_model,
                    video,
                )
                .await
                {
                    Ok(resume) => video.resume = resume,
                    Err(error) => log::error!(
                        "{}/{} - Error while running computer vision model: {}",
                        video.numeric_id,
                        self.videos_number,
                        error
                    ),
                }
            }
        }
    }

    /// Runs the text model for all the videos files based on the computer vision model's output.
    pub async fn run_resume_model(&mut self) {
        for video in &mut self.videos {
            if video.skip {
                log::info!(
                    "{}/{} - Skipping {}",
                    video.numeric_id,
                    self.videos_number,
                    video.path
                );
            } else {
                log::info!(
                    "{}/{} - Running resume model for {}",
                    video.numeric_id,
                    self.videos_number,
                    video.path
                );
                match run_resume_model_for_video(
                    &self.text_ollama,
                    &self.settings.text_model,
                    video,
                )
                .await
                {
                    Ok(resume) => {
                        log::info!(
                            "{}/{} - Title: {}",
                            video.numeric_id,
                            self.videos_number,
                            resume.title
                        );
                        log::info!(
                            "{}/{} - Description: {}",
                            video.numeric_id,
                            self.videos_number,
                            resume.description
                        );
                        log::info!(
                            "{}/{} - Keywords: {}",
                            video.numeric_id,
                            self.videos_number,
                            resume.keywords.join(", ")
                        );
                        video.resume = resume;
                    }
                    Err(error) => log::error!(
                        "{}/{} - Error while running resume model: {}",
                        video.numeric_id,
                        self.videos_number,
                        error
                    ),
                }
            }
        }
    }

    /// Exports the results of the analysis in a JSON file.
    ///
    /// ### Parameters
    /// - `path`: The path of the file to write.  
    ///   
    /// ### Returns
    /// An empty Result in case of success.
    ///
    /// ### Errors
    /// Returns an error if the export fails.
    pub fn export_to_json(&self, path: &str) -> anyhow::Result<()> {
        let contents = serde_json::to_string_pretty(&self.videos)?;
        let _ = fs::write(path, contents)?;
        Ok(())
    }

    /// Renames the videos based on the results of the analysis.
    ///
    /// ### Parameters
    /// - `template`: The template for the new file name.
    pub fn rename_videos(&mut self, template: &str) {
        self.videos.par_iter_mut().for_each(|video| {
            let new_name = file_management::create_new_file_name(video, template);
            let new_path =
                &file_management::create_new_path(video.path.as_str(), new_name.as_str());
            match file_management::rename_file(&video.path, new_path) {
                Ok(_) => log::info!(
                    "{}/{} - Renamed to: {}",
                    video.numeric_id,
                    self.videos_number,
                    new_name
                ),
                Err(error) => log::error!(
                    "{}/{} - Error while renaming file: {}",
                    video.numeric_id,
                    self.videos_number,
                    error
                ),
            }
        });
    }
}

#[derive(Debug)]
enum FrameExtractionError {
    FFMpegNotFoundError(String),
    ExtractionError(String),
}

impl std::error::Error for FrameExtractionError {}

impl fmt::Display for FrameExtractionError {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match *self {
            FrameExtractionError::FFMpegNotFoundError(ref _cause) => write!(f, "FFMpeg not found."),
            FrameExtractionError::ExtractionError(ref cause) => {
                write!(f, "Error while extracting the frames for: {}", cause)
            }
        }
    }
}

/// Extract frames for a video.
///
/// ### Parameters
/// - `temp_folder`: The path of the temp folder to save the thumbnails in.    
/// - `video`: The video that will have thumbnails extracted.
///   
/// ### Returns
/// A Result containing an array of paths to the thumbnails.
///
/// ### Errors
/// Returns an error if FFmpeg can't be run (e.g. not in the path).
fn extract_frames_for_video(temp_folder: &str, video: &Video) -> anyhow::Result<Vec<String>> {
    let mut path: PathBuf = PathBuf::from(temp_folder);
    let mut filename_template = video.id.clone();
    filename_template.push_str("_%04d.png");
    path = path.join(filename_template);
    let mut binding = Command::new("ffmpeg");
    let mut fps = String::from("fps=1/");
    fps.push_str(format!("{}", video.gap).as_str());
    let ffmpeg_command = binding
        .arg("-i")
        .arg(video.path.as_str())
        .arg("-vf")
        .arg(fps)
        .arg(path.to_str().unwrap())
        .stdout(Stdio::null())
        .stderr(Stdio::null());
    let status = ffmpeg_command.status();
    if status.is_err() {
        if status.err().unwrap().kind() == ErrorKind::NotFound {
            let error_message = "FFMpeg can't be found, we're stopping here. Please install FFMpeg and FFProbe and make sure they're in the path.".to_string();
            return Err(FrameExtractionError::FFMpegNotFoundError(error_message).into());
        } else {
            let error_message = format!("Couldn't run FFmpeg for file {}", video.path);
            return Err(FrameExtractionError::ExtractionError(error_message).into());
        }
    }

    let thumbnails = file_management::list_matching_files(temp_folder, video.id.as_str());
    Ok(thumbnails)
}

/// Runs a text model to create a resume of the video file after it's been analysed by the computer vision model.
///
/// ### Parameters
/// - `ollama`: The model prompter for the text model.    
/// - `model`: The name of the model.   
/// - `video`: The video to analyse.   
///
/// ### Returns
/// A Result containing a resume of the video.
///
/// ### Errors
/// Returns an error if the model can't be reached, doesn't exist, or doesn't return a result.
async fn run_resume_model_for_video(
    ollama: &Ollama,
    model: &str,
    video: &Video,
) -> anyhow::Result<Resume> {
    let prompt = "You are a helpful assistant and expert in concise storytelling. The following text tells the story of a video. Please resume that story in 20 words focusing on the person and their action and less on their environment, from that resume please generate a title of maximum 8 words, and make a list of up to 5 keywords that resumes the story, the keywords will include the person on the video if any (e.g. woman, child...). Please format the answer in a json format: {\"title\": <<title>>, \"description\": <<description>>, \"keywords\": <<array of keywords>>}, with no other text at all, only the json result. The story is:";
    if video.story.is_empty() {
        Err(anyhow::anyhow!("No story to resume for : {}", video.path))
    } else {
        let mut resume_prompt = prompt.to_string();
        resume_prompt += video.story.as_str();
        let options = GenerationOptions::default().temperature(0.5);
        let res = ollama
            .generate(GenerationRequest::new(model.to_string(), resume_prompt).options(options))
            .await;
        if let Ok(res) = res {
            Ok(serde_json::from_str(res.response.as_str())?)
        } else {
            Err(anyhow::anyhow!(
                "Couldn't generate answer from resume model for file: {}",
                video.path
            ))
        }
    }
}

/// Runs a computer vision model to create a story of the video file based on thumbnails of this video.
///
/// ### Parameters
/// - `ollama`: The model prompter for the computer vision model.    
/// - `model`: The name of the model.   
/// - `video`: The video to analyse.
///   
/// ### Returns
/// A Result containing a story of the video.
///
/// ### Errors
/// Returns an error if the model can't be reached, doesn't exist, or doesn't return a result.
async fn run_computer_vision_model_for_video(
    ollama: &Ollama,
    model: &str,
    video: &Video,
) -> anyhow::Result<String> {
    let prompt = "The following images are part of a video, they tell a story. Please describe that story focusing on the persons and their action and less on their environment.";

    image_resizer::resize_images(&video.thumbnails);
    let mut images = vec![];
    for thumbnail in &video.thumbnails {
        let image_data = match fs::read(thumbnail) {
            Ok(img) => img,
            Err(_) => {
                return Err(anyhow::anyhow!(
                    "Couldn't generate answer from computer vision model for file: {}",
                    video.path,
                ));
            }
        };

        // Encode the image data as Base64
        images.push(Image::from_base64(
            BASE64_STANDARD.encode(&image_data).as_str(),
        ))
    }
    let options = GenerationOptions::default().temperature(0.5);
    let res = ollama
        .generate(
            GenerationRequest::new(model.to_string(), prompt.to_string())
                .options(options)
                .images(images),
        )
        .await;
    match res {
        Ok(res) => {
            log::debug!("Story: {}", res.response);
            return Ok(res.response);
        }
        Err(err) => {
            log::debug!("Error in run_computer_vision_model_for_video: {}", err); //TODO push the error to the front
            return Err(anyhow::anyhow!(
                "Couldn't generate answer from computer vision model for file: {}",
                video.path
            ));
        }
    }
}

/// Runs a computer vision model to create a resume of the video file based on thumbnails of this video. Note that all the CV models are not able to generate the proper output at once and therefore it will be necessary to run the 2 septs with CV model than text model.
///
/// ### Parameters
/// - `ollama`: The model prompter for the computer vision model.    
/// - `model`: The name of the model.   
/// - `video`: The video to analyse.   
///
/// ### Returns
/// A Result containing a resume of the video.
///
/// ### Errors
/// Returns an error if the model can't be reached, doesn't exist, or doesn't return a result.
async fn run_only_computer_vision_model_for_video(
    ollama: &Ollama,
    model: &str,
    video: &Video,
) -> anyhow::Result<Resume> {
    let prompt = "The following images are part of a video, they tell a story. Please describe that story focusing on the persons and their action and less on their environment. Please resume that story in 20 words focusing on the person and their action and less on their environment, from that resume please generate a title of maximum 8 words, and make a list of up to 5 keywords that resumes the story, the keywords will include the person on the video if any (e.g. woman, child...). Please format the answer in a valid json format: {\"title\": <<title>>, \"description\": <<description>>, \"keywords\": <<array of keywords>>}, with no other text at all, only the json result.";

    image_resizer::resize_images(&video.thumbnails);
    let mut images = vec![];
    for thumbnail in &video.thumbnails {
        let image_data = match fs::read(thumbnail) {
            Ok(img) => img,
            Err(_) => {
                return Err(anyhow::anyhow!(
                    "Couldn't generate answer from computer vision model for file: {}",
                    video.path,
                ));
            }
        };

        // Encode the image data as Base64
        images.push(Image::from_base64(
            BASE64_STANDARD.encode(&image_data).as_str(),
        ))
    }
    let options = GenerationOptions::default().temperature(0.5);
    let res = ollama
        .generate(
            GenerationRequest::new(model.to_string(), prompt.to_string())
                .options(options)
                .images(images),
        )
        .await;
    match res {
        Ok(res) => {
            match extract_json(&res.response) {
                Some(story) => {
                    log::debug!("Story: {}", story);
                    Ok(serde_json::from_str(story.as_str())?)
                }
                None => Err(anyhow::anyhow!(
                    "Couldn't generate answer from computer vision model for file: {}",
                    video.path
                )),
            }
            //log::debug!("Story: {}", res.response);
            //return Ok(res.response);
        }
        Err(err) => {
            log::debug!("Error in run_computer_vision_model_for_video: {}", err); //TODO push the error to the front
            return Err(anyhow::anyhow!(
                "Couldn't generate answer from computer vision model for file: {}",
                video.path
            ));
        }
    }
}

/// Gets the video's metadata ia FFprobe.
///
/// ### Parameters
/// - `video_path`: The path to the video to analyse.  
///  
/// ### Returns
/// A Result containing the duration of the video and its creation date.
///
/// ### Errors
/// Returns an error if FFprobe can't be run (e.g. not in the path).
fn get_video_metadata(video_path: &str) -> anyhow::Result<(Option<f32>, Option<DateTime<Utc>>)> {
    let output = Command::new("ffprobe")
        .arg("-v")
        .arg("error")
        .arg("-show_entries")
        .arg("format=duration")
        .arg("-show_entries")
        .arg("stream_tags=creation_time")
        .arg("-of")
        .arg("default=noprint_wrappers=1:nokey=1")
        .arg(video_path)
        .output()?;

    if !output.status.success() {
        return Err(anyhow::Error::msg(format!(
            "Couldn't run FFprobe for file {}",
            video_path
        )));
    }

    let binding = String::from_utf8(output.stdout).unwrap();
    let mut set = HashSet::new();
    let metadata: Vec<String> = binding
        .trim()
        .lines()
        .map(|s| s.to_string())
        .filter(|item| set.insert(item.clone()))
        .collect();

    Ok(parse_metadata_to_tuple(metadata))
}

/// Parses the result from FFprobe into something usable.
///
/// ### Parameters
/// - `values`: The raw values from FFprobe.   
///
/// ### Returns
/// A tuple with the duration of the video and its creation date.
fn parse_metadata_to_tuple(values: Vec<String>) -> (Option<f32>, Option<DateTime<Utc>>) {
    let mut float_opt: Option<f32> = None;
    let mut date_opt: Option<DateTime<Utc>> = None;

    for value in values {
        // Try parsing as a DateTime first
        if let Ok(date) = DateTime::parse_from_rfc3339(&value) {
            date_opt = Some(date.to_utc());
        } else if let Ok(num) = value.parse::<f32>() {
            // Not a date, try parsing as a float
            float_opt = Some(num);
        }
    }

    (float_opt, date_opt)
}

/// Gets the gap between two thumbnails extractions in seconds.
///
/// ### Parameters
/// - `duration`: The duration of the video.  
///  
/// ### Returns
/// The gap between two thumbnails extractions in seconds.
fn get_capture_gap(duration: f32) -> i32 {
    let f_gap = duration / 3.0;
    f_gap.floor() as i32
}

/// Extracts the JSON value in the model's result in the case noise is introduced.
///
/// ### Parameters
/// - `input`: The model's result.   
///
/// ### Returns
/// The extracted JSON.
fn extract_json(input: &str) -> Option<String> {
    // This regex looks for the JSON pattern, assuming no curly braces in strings within the JSON
    let re = Regex::new(r"\{(?:[^{}]*|(?R))*\}").unwrap();
    re.find(input).map(|mat| mat.as_str().to_string())
}
